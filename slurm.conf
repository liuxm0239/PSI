# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=slurmsched1
ControlAddr=10.0.0.49
BackupController=slurmsched2
BackupAddr=10.0.0.50
#
AuthType=auth/munge
CacheGroups=0
#CheckpointType=checkpoint/none
CryptoType=crypto/munge
#DisableRootJobs=NO
EnforcePartLimits=YES
#Epilog=
#EpilogSlurmctld=
#FirstJobId=1
#MaxJobId=999999
MaxArraySize=30001
GresTypes=gpu
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobCheckpointDir=/var/slurm/checkpoint
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=1
#KillOnBadExit=0
#LaunchType=launch/slurm
#Licenses=stata_cas:5
#MailProg=/bin/mail
#MaxJobCount=48000  # WRF, increasing to 150000 since we actually bumped up against 84000 today. 3/24/16
MaxJobCount=150000
#MaxStepCount=40000
#MaxTasksPerNode=128
MpiDefault=none
#MpiParams=ports=#-#
PluginDir=/usr/scheduler/slurm/lib/slurm
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/cgroup
#Prolog=
#PrologFlags=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
PropagateResourceLimitsExcept=MEMLOCK
RebootProgram=/sbin/reboot
ReturnToService=1
SallocDefaultCommand="srun --mem-per-cpu=0 --pty $SHELL"
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurm/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/usr/spool/slurm
SlurmUser=slurm
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/usr/scheduler/state
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/cgroup
#TaskPluginParam=
#TaskProlog=
#TopologyPlugin=topology/tree
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0
#
#
# TIMERS
BatchStartTimeout=30
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
MessageTimeout=90
#ResvOverRun=0
MinJobAge=150
#OverTimeLimit=0
#SlurmctldTimeout=3600
#SlurmdTimeout=3600
SlurmctldTimeout=120
#SlurmdTimeout=300
SlurmdTimeout=600    # CNJ -- raised per slurmctld log file entry on 20160201
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
#
# SCHEDULING
#DefMemPerCPU=3000
FastSchedule=1
#MaxMemPerCPU=
#SchedulerRootFilter=1
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SchedulerPort=7321
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory
SchedulerParameters=bf_interval=120,bf_continue,bf_resolution=300,bf_max_job_test=4000,bf_max_job_user=20,max_sched_time=2,max_rpc_cnt=160
PreemptType=preempt/qos
PreemptMode=CANCEL
#
#
# JOB PRIORITY
#PriorityFlags=
PriorityType=priority/multifactor
PriorityDecayHalfLife=2-0
PriorityUsageResetPeriod=NONE
#PriorityCalcPeriod=
#PriorityFavorSmall=
PriorityMaxAge=14-0
PriorityWeightAge=25000
PriorityWeightFairshare=100000
PriorityWeightJobSize=1000
#PriorityWeightPartition=1000
PriorityWeightQOS=100000000
#
#
# LOGGING AND ACCOUNTING
AccountingStorageEnforce=safe,qos
AccountingStorageHost=slurmdb
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageUser=slurm
AccountingStoreJobComment=YES
AccountingStorageTRES=gres/gpu
ClusterName=accre
#DebugFlags=NO_CONF_HASH
JobCompHost=slurmdb
JobCompLoc=http://testaburger.vampire:9200
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/elasticsearch
#JobCompUser=slurm
#JobContainerType=job_container/none
#JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
#SlurmctldDebug=7
SlurmctldDebug=3
#SlurmctldDebug=debug2
#SlurmctldLogFile=
#SlurmdDebug=7
SlurmdDebug=3
#SlurmdLogFile=
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
FirstJobId=9700000
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=




####################
# COMPUTE NODES 
#   * WARNING: Don't forget to also add/remove nodes to the appropriate Partition below!
####################

# RACK E10
# RACK E18
NodeName=gpu[0001-0010] RealMemory=128000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=1 Gres=gpu:4 Feature=e18,twelve,haswell,maxwell
# RACK E19
NodeName=gpu[0011-0012] RealMemory=128000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=1 Gres=gpu:4 Feature=e19,twelve,haswell,maxwell
NodeName=gpu[0013-0020] RealMemory=128000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Gres=gpu:4 Feature=e19,eight,broadwell,pascal
# RACK E20
NodeName=gpu[0021-0022] RealMemory=128000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Gres=gpu:4 Feature=e20,eight,broadwell,pascal
# RACK E21
NodeName=vmp[807,825,826] RealMemory=45000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Gres=gpu:4 Feature=e21,eight,westmere,fermi
NodeName=vmp[815,834,836-838] RealMemory=45000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Gres=gpu:4 Feature=e21,eight,westmere,fermi
# RACK E22
NodeName=vmp[818,844] RealMemory=45000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Gres=gpu:4 Feature=e22,eight,westmere,fermi
NodeName=vmp[902-905] RealMemory=123000 CPUs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=e22,sixteen,haswell,mic
# RACK H06
NodeName=vmp[1201-1206,1208-1216,1218-1229,1232-1235] RealMemory=123000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=h06,twelve,haswell Weight=66
NodeName=vmp[1207,1217] RealMemory=104000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=h06,twelve,haswell Weight=66
# RACK H07
NodeName=vmp[1236-1242,1257] RealMemory=123000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=h07,twelve,haswell Weight=66
NodeName=vmp[1258,1260,1261,1263,1266-1271,1273-1279,1284-1287,1289-1290] RealMemory=123000 CPUs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=h07,sixteen,haswell Weight=66
NodeName=vmp[1259,1262,1264,1265,1272,1280-1283,1288] RealMemory=252000 CPUs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=h07,sixteen,haswell,bigmem Weight=71
# RACK H11
NodeName=vmp[623] RealMemory=40000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h11,eight,nehalem Weight=11
# RACK H12
NodeName=vmp[591-598] RealMemory=92000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h12,eight,nehalem Weight=56
NodeName=vmp[588] RealMemory=45000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h12,eight,nehalem Weight=1
# RACK H14 (fabrication 591)
Nodename=vmp[1331-1354,1367-1368] RealMemory=123000 CPUs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=h14,sixteen,haswell Weight=61
Nodename=vmp[1355-1366] RealMemory=252000 CPUs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=h14,sixteen,haswell Weight=71
# RACK H15 (fabrication 590)
Nodename=vmp[1300-1321] RealMemory=123000 CPUs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=h14,sixteen,haswell Weight=61
Nodename=vmp[1293-1299,1322-1330] RealMemory=252000 CPUs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=h15,sixteen,haswell Weight=71
# RACK H18
NodeName=vmp[666-671,679,681-686,688-690] RealMemory=20500 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h18,eight,nehalem Weight=1
NodeName=vmp[692,695-698] RealMemory=92000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h18,eight,nehalem Weight=56
# RACK H19
NodeName=vmp[201-216] RealMemory=45000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=h19,twelve,westmere Weight=21
NodeName=vmp[729-736] RealMemory=20500 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h19,eight,nehalem Weight=1
# RACK H20
NodeName=vmp[1001-1003,1007-1030,1032-1039] RealMemory=20500 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h20,eight,westmere Weight=1
NodeName=vmp[1031] RealMemory=16000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h20,eight,westmere Weight=1
NodeName=vmp[1040] RealMemory=20500 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h20,eight,westmere Weight=1
# RACK H21
NodeName=vmp[1041-1059] RealMemory=45000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=h21,twelve,westmere Weight=11
NodeName=vmp[1061-1080] RealMemory=20500 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h21,eight,westmere Weight=1
# RACK H22
NodeName=vmp[217-238] RealMemory=123000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h22,eight,westmere Weight=51
NodeName=vmp[239] RealMemory=123000 CPUs=8 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Feature=h22,twelve,sandy_bridge Weight=51
NodeName=vmp[1121-1135] RealMemory=60000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=h22,twelve,sandy_bridge Weight=41
NodeName=vmp[1255-1256] RealMemory=123000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=h22,twelve,sandy_bridge Weight=66
# RACK H23
NodeName=vmp[1291-1292] RealMemory=123000 CPUs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=h23,sixteen,haswell Weight=61
# RACK L22
NodeName=vmp[1081-1096] RealMemory=60000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=l22,twelve,sandy_bridge Weight=41
NodeName=vmp[1097-1120] RealMemory=60000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=l22,twelve,westmere Weight=41
# RACK L25
NodeName=vmp[381-382] RealMemory=252000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=l25,twelve,sandy_bridge,bigmem Weight=76
NodeName=vmp[383-415,417-420] RealMemory=123000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=l25,twelve,sandy_bridge Weight=66
NodeName=vmp[416] RealMemory=92000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=l25,twelve,sandy_bridge Weight=61
# RACK L26
NodeName=vmp[423-448,450-460] RealMemory=123000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=l26,twelve,sandy_bridge Weight=66
NodeName=vmp[449] RealMemory=92000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=l26,twelve,sandy_bridge Weight=61
NodeName=vmp[421,422] RealMemory=252000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=l26,twelve,sandy_bridge,bigmem Weight=76
# RACK O20
NodeName=vmp[341-380] RealMemory=123000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=o20,twelve,sandy_bridge Weight=66
# RACK O21
NodeName=vmp[301-340] RealMemory=123000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=o21,twelve,sandy_bridge Weight=66
# RACK O26
NodeName=vmp[464-500] RealMemory=123000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=o26,twelve,sandy_bridge Weight=66
NodeName=vmp[461,462,463] RealMemory=252000 CPUs=12 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=o26,twelve,sandy_bridge,bigmem Weight=76
# hv-qa
NodeName=vm-qa-node[001-004] RealMemory=3000 CPUs=2 Sockets=2 CoresPerSocket=1 ThreadsPerCore=1 Feature=hv-qa
####################
# PARTITIONS
####################

# Standard compute nodes go in production partition
PartitionName=production Nodes=vmp[201,202,203-239,301-500,591-598,666-671,679,681-686,688-690,692,695-698,729-736,1001-1003,1007-1059,1061-1135,1201-1229,1232-1242,1255-1368] Default=YES MaxTime=20160 DefaultTime=15 DefMemPerCPU=1000 MaxMemPerCPU=21000 State=UP Shared=NO DenyAccounts=accre_gpu,aivas_lab_gpu,anderson_mri_gpu,beam_lab_gpu,biostat_gpu,capra_lab_gpu,cerl_gpu,cgg_gpu,chbe285_gpu,cms_gpu,csb_gpu,cummings_gpu,dbmi_gpu,grissom_lab_gpu,guest_gpu_account,haines_gpu,holmes_lab_gpu,h_cutting_gpu,h_fabbri_gpu,h_lasko_lab_gpu,h_vmac_gpu,h_vuni_gpu,johnston_group_gpu,lola_gpu,lss_gpu,lybrand_gpu,math_gpu,mccabe_gpu,mip_eecs_gpu,nbody_gpu,nbody_gpu_account,palmeri_gpu,palmeri_gpu_account,polavarapu_gpu,photogrammetry_gpu,polyn_gpu,p_malin_gpu,p_masi_gpu,p_meiler_gpu,p_meiler_gpu_account,p_neuert_gpu,rokas_gpu,tplab_gpu,tplab_gpu_account,zhao_gpu,edwards_lab_mic,grissom_lab_mic,guest_mic_account,holmes_lab_mic,h_biostat_student_mic,lola_mic,segrest_group_gpu,tplab_mic

# Debug partition; QOS overrides normal association limits 
PartitionName=debug Nodes=vmp588,vm-qa-node[001-004] Default=NO MaxTime=120 DefaultTime=15 DefMemPerCPU=1000 MaxMemPerCPU=20500 State=UP Shared=NO DenyAccounts=accre_gpu,aivas_lab_gpu,anderson_mri_gpu,beam_lab_gpu,biostat_gpu,capra_lab_gpu,cerl_gpu,cgg_gpu,chbe285_gpu,cms_gpu,csb_gpu,cummings_gpu,dbmi_gpu,grissom_lab_gpu,guest_gpu_account,haines_gpu,holmes_lab_gpu,h_cutting_gpu,h_fabbri_gpu,h_lasko_lab_gpu,h_vmac_gpu,h_vuni_gpu,johnston_group_gpu,lola_gpu,lss_gpu,lybrand_gpu,math_gpu,mccabe_gpu,mip_eecs_gpu,nbody_gpu,nbody_gpu_account,palmeri_gpu,palmeri_gpu_account,photogrammetry_gpu,polavarapu_gpu,polyn_gpu,p_malin_gpu,p_masi_gpu,p_meiler_gpu,p_meiler_gpu_account,p_neuert_gpu,rokas_gpu,tplab_gpu,tplab_gpu_account,zhao_gpu,edwards_lab_mic,grissom_lab_mic,guest_mic_account,holmes_lab_mic,h_biostat_student_mic,lola_mic,segrest_group_gpu,tplab_mic QOS=debug

# GPU nodes in separate partitions, one for each GPU type
PartitionName=fermi Nodes=vmp[807,815,818,825,826,834,836-838,844] Default=NO MaxTime=20160 DefaultTime=15 DefMemPerNode=2000 MaxMemPerNode=45000 State=UP AllowAccounts=accre_gpu,aivas_lab_gpu,anderson_mri_gpu,beam_lab_gpu,biostat_gpu,capra_lab_gpu,cerl_gpu,cgg_gpu,chbe285_gpu,cms_gpu,csb_gpu,cummings_gpu,dbmi_gpu,guest_gpu_account,grissom_lab_gpu,haines_gpu,holmes_lab_gpu,h_cutting_gpu,h_fabbri_gpu,h_lasko_lab_gpu,h_vmac_gpu,h_vuni_gpu,johnston_group_gpu,lola_gpu,lss_gpu,lybrand_gpu,math_gpu,mccabe_gpu,mip_eecs_gpu,nbody_gpu,nbody_gpu_account,palmeri_gpu,palmeri_gpu_account,photogrammetry_gpu,polavarapu_gpu,polyn_gpu,p_malin_gpu,p_masi_gpu,p_meiler_gpu,p_meiler_gpu_account,p_neuert_gpu,rokas_gpu,segrest_group_gpu,tplab_gpu,tplab_gpu_account,zhao_gpu
PartitionName=maxwell Nodes=gpu[0001-0012] Default=NO MaxTime=7200 DefaultTime=15 DefMemPerNode=2000 MaxMemPerNode=124000 State=UP AllowAccounts=accre_gpu,aivas_lab_gpu,anderson_mri_gpu,beam_lab_gpu,biostat_gpu,capra_lab_gpu,cerl_gpu,cgg_gpu,chbe285_gpu,cms_gpu,csb_gpu,cummings_gpu,dbmi_gpu,grissom_lab_gpu,guest_gpu_account,haines_gpu,holmes_lab_gpu,h_cutting_gpu,h_fabbri_gpu,h_lasko_lab_gpu,h_vmac_gpu,h_vuni_gpu,johnston_group_gpu,lola_gpu,lss_gpu,lybrand_gpu,math_gpu,mccabe_gpu,mip_eecs_gpu,nbody_gpu,nbody_gpu_account,palmeri_gpu,palmeri_gpu_account,photogrammetry_gpu,polavarapu_gpu,polyn_gpu,p_malin_gpu,p_masi_gpu,p_meiler_gpu,p_meiler_gpu_account,p_neuert_gpu,rokas_gpu,sc3260,segrest_group_gpu,tplab_gpu,tplab_gpu_account,zhao_gpu
PartitionName=pascal Nodes=gpu[0013-0022] Default=NO MaxTime=7200 DefaultTime=15 DefMemPerNode=2048 MaxMemPerNode=126976 State=UP AllowAccounts=accre_gpu

# Phi nodes also in separte partiion with nodes not shared between jobs (for now); users only have access if association added to SLURM db 
PartitionName=mic Nodes=vmp[902-905] Default=NO MaxTime=20160 DefaultTime=15 DefMemPerNode=16000 MaxMemPerNode=123000 State=UP Shared=EXCLUSIVE AllowAccounts=accre,edwards_lab_mic,grissom_lab_mic,guest_mic_account,holmes_lab_mic,h_biostat_student_mic,lola_mic,sc3260,tplab_mic


